{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blogger_scoring_colab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuGaiDf3BaDxqcOn5pJyqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zambbo/BERT-blog-sentiment-analysis/blob/master/blogger_scoring_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "id": "L9_1Jie7p0vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "id": "1TQQWNz5qC4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bW-r4fEYF8d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS-L3icvph9x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from kobert import get_tokenizer\n",
        "from kobert import get_pytorch_kobert_model\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import kss\n",
        "import re\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "### 아래의 경로들은 환경에 따라 바꿔줘야한다.\n",
        "# 모델이 저장되어 있는 경로\n",
        "sent_path = '/content/drive/MyDrive/dataset/sentiment_model.pt'\n",
        "subj_path = '/content/drive/MyDrive/dataset/subjective_model.pt'\n",
        "# 블로거의 블로거 글들이 저장되어있는 폴더\n",
        "root_file_path = \"/content/drive/MyDrive/blogger detail\"\n",
        "# 최종 스코어링 한 csv파일을 저장할 폴더\n",
        "result_file_path = \"/content/drive/MyDrive/After Scoring\"\n",
        "\n",
        "\n",
        "\n",
        "#데이터 셋을 BERT모델에 알맞은 형태로 변경\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "#BERT 모델\n",
        "#sentiment predict를 위해서는 num_classes를 3으로 설정해준다\n",
        "#objective/subjective predict를 위해서는 num_classes를 2로 설정해준다.\n",
        "#default는 3으로 되어있다\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=3,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "        \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "    \n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)        \n",
        "\n",
        "\n",
        "def load_model(model_path, num_classes:int = 2):\n",
        "    model = BERTClassifier(bertmodel, num_classes=num_classes, dr_rate=0.5).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class Scorer:\n",
        "\n",
        "    def __init__(self, blogger_name, sentiment_model, subjective_model, tokenizer):\n",
        "        file_path = os.path.join(root_file_path,f\"{blogger_name}.csv\")\n",
        "        #file_path = f\"./blogger detail/{blogger_name}.csv\"\n",
        "        self.blogger_name = blogger_name\n",
        "        try:\n",
        "            self.df=  pd.read_csv(file_path, index_col = 0)\n",
        "        except:\n",
        "            raise Exception(\"no blogger name\")\n",
        "        self.tok = tokenizer\n",
        "        self.sentiment_model = sentiment_model\n",
        "        self.subjective_model = subjective_model\n",
        "\n",
        "    def row_func(self, row):\n",
        "        year, month, day, _ = row.split(\".\")\n",
        "        return datetime.date(int(year), int(month), int(day))\n",
        "    \n",
        "    def preprocessing(self, sentence):\n",
        "        sentence = re.sub(\"[^ ㄱ-ㅎ가-힣a-zA-Z0-9]\",\"\",sentence)\n",
        "        return sentence\n",
        "\n",
        "    def split_post(self, post):\n",
        "        sentences = []\n",
        "        for sentence in kss.split_sentences(post):\n",
        "            sentences.append(sentence)\n",
        "        return sentences\n",
        "\n",
        "    def bertToTorch(self, _data, device):\n",
        "        token_ids, valid_length, segment_ids, label = _data\n",
        "        token_ids = torch.tensor([token_ids])\n",
        "        segment_ids = torch.tensor([segment_ids])\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length = torch.IntTensor([valid_length.item()])\n",
        "        return token_ids, valid_length, segment_ids\n",
        "\n",
        "    def toBertDataset(self, post):\n",
        "        max_len = 64\n",
        "        post = [[sent,'1'] for sent in post]\n",
        "        bertdataset = BERTDataset(post, 0, 1, self.tok, max_len, True, False)\n",
        "        return bertdataset\n",
        "\n",
        "    def scoring_func(self, _predicted):\n",
        "        predicted = _predicted.cpu().detach().numpy()\n",
        "        predicted_value = np.argmax(predicted)\n",
        "        return predicted_value\n",
        "\n",
        "    def predict_label(self, _bertdataset, _model, score_func):\n",
        "        predicted_labels = []\n",
        "        for bs in _bertdataset:\n",
        "            token_ids, valid_length, segment_ids = self.bertToTorch(bs, device)\n",
        "            predicted = _model(token_ids, valid_length, segment_ids)\n",
        "            predicted_value = score_func(predicted)\n",
        "            predicted_labels.append(predicted_value)\n",
        "        \n",
        "        predicted_labels = predicted_labels\n",
        "        return predicted_labels\n",
        "\n",
        "    def scoring(self):\n",
        "        df = self.df\n",
        "        dates = df['Posting Date']\n",
        "        dates = dates.apply(self.row_func)\n",
        "\n",
        "        df['Posting Date'] = dates\n",
        "        df = df.sort_values(by=['Posting Date'], ascending=False)\n",
        "\n",
        "        if len(df) > 90:\n",
        "            df = df[:90]\n",
        "\n",
        "        posts = df['Post']\n",
        "        posts = posts.apply(self.preprocessing)\n",
        "        df['Post'] = posts\n",
        "        \n",
        "        posts = df['Post'].apply(self.split_post)\n",
        "        posts_sentiments = []\n",
        "        posts_subjectives = []\n",
        "\n",
        "        for post in posts:\n",
        "\n",
        "            post_bert = self.toBertDataset(post)\n",
        "            predicted_sentiments = self.predict_label(post_bert, self.sentiment_model, self.scoring_func)\n",
        "            predicted_subjectives = self.predict_label(post_bert, self.subjective_model, self.scoring_func)\n",
        "\n",
        "            posts_sentiments.append(predicted_sentiments)\n",
        "            posts_subjectives.append(predicted_subjectives)\n",
        "\n",
        "        posts = posts.values\n",
        "        after_predicted = []\n",
        "        for post, sentiments, subjectives in zip(posts, posts_sentiments, posts_subjectives):\n",
        "            after_predicted.append([(sentence, sentiment, subjective) for sentence, sentiment, subjective in zip(post, sentiments, subjectives)])\n",
        "\n",
        "        df['AP'] = after_predicted\n",
        "\n",
        "        file_path = os.path.join(result_file_path,f'{self.blogger_name}.csv')\n",
        "        df.to_csv(file_path)\n",
        "        print(f\"{self.blogger_name} saving finish!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KoBERT 모델과 KoBERT vocabulary를 다운받는다.\n",
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
        "\n",
        "#전체 블로거 닉네임 리스트\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset/recommender_final.csv',index_col=0)\n",
        "blogger_list = list(df.index)\n",
        "sentiment_model = load_model(sent_path, 3)\n",
        "subjective_model = load_model(subj_path, 2)\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False) \n",
        "for blogger_name in blogger_list:\n",
        "    try:\n",
        "        scorer = Scorer(blogger_name, sentiment_model, subjective_model, tok)\n",
        "        scorer.scoring()\n",
        "    except Exception as e :\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "ptNGaWAYpqa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AOqiND2hRtbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}