{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(model, new_sentence, tokenizer):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True)\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    pad_new = pad_sequences(encoded, maxlen = 50)\n",
    "    pad_new = np.array(pad_new)\n",
    "    score = np.argmax(model.predict(pad_new))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labeling(filename, sentences, sentiments, objectives):\n",
    "    with open(filename,\"w\",encoding=\"utf-8\") as f:\n",
    "        for sentence, sentiment, objective in zip(sentences, sentiments, objectives):\n",
    "            f.write(f\"{sentence}\\t{sentiment}\\t{objective}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, y, output_dim, optimizer_function, loss_function, activation_function):\n",
    "    embedding_dim = 100\n",
    "    hidden_units = 128\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(50000, embedding_dim, input_length=X.shape[1]))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    model.add(Dense(output_dim, activation=activation_function))\n",
    "\n",
    "    model.compile(optimizer=optimizer_function, loss=loss_function, metrics=['acc'])\n",
    "    history = model.fit(X, y, epochs=15, batch_size=64, validation_split=0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_data(X, tokenizer, labeling_index):\n",
    "    X = X[:labeling_index]\n",
    "    X = [re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",sent) for sent in X]\n",
    "    X = [okt.morphs(sent, stem=True) for sent in X]\n",
    "    X = [sent for sent in X if not sent in stopwords]\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=50)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y_data(y, labeling_index):\n",
    "    y = y[:labeling_index]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_path(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t' ,names=['sentence','sentiment','objective'])\n",
    "    sentences = df['sentence'].values\n",
    "    sentiments = df['sentiment'].values\n",
    "    objectives = df['objective'].values\n",
    "    return (sentences, sentiments, objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentiments, objectives = get_data_from_path(\"./data/new_labeling2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    stopword_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
    "    X_train.append(stopword_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(50000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_len = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = make_X_data(sentences, tokenizer, t_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentiments = make_y_data(sentiments, t_len)\n",
    "training_objectives = make_y_data(objectives, t_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentiments = np.nan_to_num(training_sentiments,nan = 1)\n",
    "training_sentiments = training_sentiments.astype(int)\n",
    "training_sentiments[:int(len(training_sentiments)/4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_objectives[np.isnan(training_objectives)] = 1\n",
    "training_objectives = training_objectives.astype(int)\n",
    "training_objectives[:int(len(training_objectives)/4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_y_train(y, num_of_classes: int):\n",
    "    new_y = np.zeros((y.shape[0], num_of_classes))\n",
    "    for idx, value in enumerate(y):\n",
    "        new_y[idx][value] = 1\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentiments = get_multi_y_train(training_sentiments,3)\n",
    "training_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "99/99 [==============================] - 39s 293ms/step - loss: 0.8288 - acc: 0.6237 - val_loss: 0.6721 - val_acc: 0.7376\n",
      "Epoch 2/15\n",
      "99/99 [==============================] - 25s 257ms/step - loss: 0.5185 - acc: 0.8018 - val_loss: 0.5831 - val_acc: 0.7541\n",
      "Epoch 3/15\n",
      "99/99 [==============================] - 26s 264ms/step - loss: 0.3174 - acc: 0.8886 - val_loss: 0.5845 - val_acc: 0.7814\n",
      "Epoch 4/15\n",
      "99/99 [==============================] - 26s 263ms/step - loss: 0.1997 - acc: 0.9382 - val_loss: 0.6406 - val_acc: 0.7757\n",
      "Epoch 5/15\n",
      "99/99 [==============================] - 26s 260ms/step - loss: 0.1504 - acc: 0.9518 - val_loss: 0.6805 - val_acc: 0.7681\n",
      "Epoch 6/15\n",
      "99/99 [==============================] - 26s 263ms/step - loss: 0.1072 - acc: 0.9699 - val_loss: 0.8100 - val_acc: 0.7731\n",
      "Epoch 7/15\n",
      "99/99 [==============================] - 26s 259ms/step - loss: 0.0946 - acc: 0.9718 - val_loss: 0.8437 - val_acc: 0.7579\n",
      "Epoch 8/15\n",
      "99/99 [==============================] - 27s 275ms/step - loss: 0.0755 - acc: 0.9778 - val_loss: 0.8481 - val_acc: 0.7560\n",
      "Epoch 9/15\n",
      "99/99 [==============================] - 26s 263ms/step - loss: 0.0622 - acc: 0.9815 - val_loss: 0.9737 - val_acc: 0.7554\n",
      "Epoch 10/15\n",
      "99/99 [==============================] - 27s 274ms/step - loss: 0.0561 - acc: 0.9829 - val_loss: 1.0556 - val_acc: 0.7433\n",
      "Epoch 11/15\n",
      "99/99 [==============================] - 26s 262ms/step - loss: 0.0696 - acc: 0.9805 - val_loss: 0.9895 - val_acc: 0.7510\n",
      "Epoch 12/15\n",
      "99/99 [==============================] - 26s 261ms/step - loss: 0.0414 - acc: 0.9897 - val_loss: 1.1445 - val_acc: 0.7446\n",
      "Epoch 13/15\n",
      "99/99 [==============================] - 26s 266ms/step - loss: 0.0358 - acc: 0.9903 - val_loss: 1.1392 - val_acc: 0.7376\n",
      "Epoch 14/15\n",
      "99/99 [==============================] - 23s 230ms/step - loss: 0.0213 - acc: 0.9949 - val_loss: 1.3395 - val_acc: 0.7503\n",
      "Epoch 15/15\n",
      "99/99 [==============================] - 22s 221ms/step - loss: 0.0162 - acc: 0.9960 - val_loss: 1.3837 - val_acc: 0.7433\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = training(training_sentences, training_sentiments, 3, 'adam', 'categorical_crossentropy', \"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_predict(sentiment_model,\"안녕하세요 정말 좋아요\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 2, 2]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_sentiments = [sentiment_predict(sentiment_model, sent, tokenizer) for sent in sentences]\n",
    "predicted_sentiments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "99/99 [==============================] - 15s 132ms/step - loss: 0.5138 - acc: 0.7546 - val_loss: 0.3918 - val_acc: 0.8226\n",
      "Epoch 2/15\n",
      "99/99 [==============================] - 12s 124ms/step - loss: 0.3916 - acc: 0.8511 - val_loss: 0.3506 - val_acc: 0.8504\n",
      "Epoch 3/15\n",
      "99/99 [==============================] - 13s 128ms/step - loss: 0.2563 - acc: 0.8994 - val_loss: 0.3272 - val_acc: 0.8580\n",
      "Epoch 4/15\n",
      "99/99 [==============================] - 13s 129ms/step - loss: 0.2090 - acc: 0.9224 - val_loss: 0.3218 - val_acc: 0.8714\n",
      "Epoch 5/15\n",
      "99/99 [==============================] - 13s 126ms/step - loss: 0.1699 - acc: 0.9373 - val_loss: 0.3486 - val_acc: 0.8561\n",
      "Epoch 6/15\n",
      "99/99 [==============================] - 13s 132ms/step - loss: 0.1464 - acc: 0.9479 - val_loss: 0.3722 - val_acc: 0.8561\n",
      "Epoch 7/15\n",
      "99/99 [==============================] - 13s 130ms/step - loss: 0.1245 - acc: 0.9574 - val_loss: 0.4034 - val_acc: 0.8479\n",
      "Epoch 8/15\n",
      "99/99 [==============================] - 13s 130ms/step - loss: 0.1052 - acc: 0.9615 - val_loss: 0.5357 - val_acc: 0.8302\n",
      "Epoch 9/15\n",
      "99/99 [==============================] - 14s 142ms/step - loss: 0.0875 - acc: 0.9689 - val_loss: 0.5097 - val_acc: 0.8397\n",
      "Epoch 10/15\n",
      "99/99 [==============================] - 14s 143ms/step - loss: 0.0729 - acc: 0.9743 - val_loss: 0.5172 - val_acc: 0.8289\n",
      "Epoch 11/15\n",
      "99/99 [==============================] - 14s 145ms/step - loss: 0.0645 - acc: 0.9772 - val_loss: 0.5750 - val_acc: 0.8359\n",
      "Epoch 12/15\n",
      "99/99 [==============================] - 14s 143ms/step - loss: 0.0545 - acc: 0.9821 - val_loss: 0.6289 - val_acc: 0.8289\n",
      "Epoch 13/15\n",
      "99/99 [==============================] - 13s 135ms/step - loss: 0.0441 - acc: 0.9838 - val_loss: 0.7228 - val_acc: 0.8251\n",
      "Epoch 14/15\n",
      "99/99 [==============================] - 14s 142ms/step - loss: 0.0420 - acc: 0.9856 - val_loss: 0.7203 - val_acc: 0.8378\n",
      "Epoch 15/15\n",
      "99/99 [==============================] - 14s 145ms/step - loss: 0.0353 - acc: 0.9880 - val_loss: 0.8096 - val_acc: 0.7953\n"
     ]
    }
   ],
   "source": [
    "objective_model = training(training_sentences, training_objectives, 1, 'rmsprop', 'binary_crossentropy', \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_predict(model, new_sentence, tokenizer):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True)\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    pad_new = pad_sequences(encoded, maxlen = 50)\n",
    "    pad_new = np.array(pad_new)\n",
    "    score = model.predict(pad_new)\n",
    "    return 1 if score > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_objectives = [objective_predict(objective_model, sent, tokenizer) for sent in sentences]\n",
    "predicted_objectives[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_labeling(\"./data/new_labeling3.txt\", sentences, predicted_sentiments, predicted_objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdfda138ad5bcab69f3640f79512bb442e262eb8606e188c4172350e36929680"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
